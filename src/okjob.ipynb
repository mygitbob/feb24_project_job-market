{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_host = '127.0.0.1'\n",
    "mongo_port = '27017'\n",
    "mongo_db = 'raw_data'\n",
    "connection_string = f\"mongodb://{mongo_host}:{mongo_port}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = 'okjob_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(connection_string)\n",
    "db = client[mongo_db]\n",
    "collection = db[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(db['okjob_test'].find_one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"$and\": [\n",
    "        {\"sourceId\": {\"$ne\": \"\"}},\n",
    "        {\"Job-Title\": {\"$ne\": \"\"}},  \n",
    "        {\"Location\": {\"$ne\": \"\"}},\n",
    "        {\"Salary-Min\": {\"$ne\": \"\"}},\n",
    "        {\"Salary-Max\": {\"$ne\": \"\"}},\n",
    "        {\"LinkedIn-Job-Link\": {\"$ne\": \"\"}},\n",
    "        {\"Job-Type\": {\"$ne\": \"\"}},\n",
    "        {\"Job-Tags\": {\"$ne\": \"\"}},\n",
    "        {\"Job-Category\": {\"$regex\": \"data\", \"$options\": \"i\"}} # Case-insensitive search for \"data\" in jobTitle\n",
    "      # {\"currency\": {\"$ne\": \"\"}}\n",
    "    ]\n",
    "}\n",
    "projection = {\n",
    "    \"sourceId\": 1,\n",
    "    \"Job-Title\": 1,\n",
    "    \"Location\": 1,\n",
    "    \"Salary-Min\": 1,\n",
    "    \"Salary-Max\": 1,\n",
    "    \"LinkedIn-Job-Link\": 1,\n",
    "    \"Job-Type\": 1,\n",
    "    \"Job-Tags\": 1,\n",
    "    \"Job-Category\": 1,\n",
    "    \"_id\": 0\n",
    "}\n",
    "\n",
    "documents = collection.find(query, projection)\n",
    "data_objects = []\n",
    "for doc in documents:\n",
    "    data_objects.append(doc)\n",
    "df_ok = pd.DataFrame(data_objects)\n",
    "df_ok['Salary-Min'] = pd.to_numeric(df_ok['Salary-Min'], errors='coerce')\n",
    "df_ok['Salary-Max'] = pd.to_numeric(df_ok['Salary-Max'], errors='coerce')\n",
    "# didn't dropna or filter salary-min, otherwise the dataset is very small, we should fill them with mean/mode later\n",
    "df_ok.head(5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_ok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize job titles\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def categorize_seniority(job_title):\n",
    "    doc = nlp(job_title)\n",
    "    # List of keywords (lemmas) to look for\n",
    "    keywords_senior = ['strategic', 'principal', 'staff', 'lead', 'senior', 'head']\n",
    "    keywords_junior = ['trainee', 'junior', 'apprentice', 'entry level' ]\n",
    "    # Check if any token's lemma is in our keywords list\n",
    "    if any(token.lemma_.lower() in keywords_senior for token in doc):\n",
    "        return 'Senior'\n",
    "    elif any(token.lemma_.lower() in keywords_junior for token in doc):\n",
    "        return 'Junior'\n",
    "    else:\n",
    "        return 'Any'\n",
    "\n",
    "# Apply the function to create a new column\n",
    "df_ok['jobLevel'] = df_ok['Job-Title'].apply(categorize_seniority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ok['Job-Category'].unique) \n",
    "print(df_ok['Job-Tags'].unique) \n",
    "print(df_ok['Job-Type'].unique) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your lists of skills, technologies, and site preferences\n",
    "\n",
    "keywords_skills = [\n",
    "    \"SQL\", \"Structured Query Language\", \"Python\", \"R\", \"Docker\", \"AWS\", \"Amazon Web Services\",\n",
    "    \"Azure\", \"Google Cloud Platform\", \"GCP\", \"Snowflake\", \"Hadoop\", \"Spark\", \"Kubernetes\",\n",
    "    \"Jenkins\", \"BI\", \"Business Intelligence\", \"Tableau\", \"Power BI\", \"Looker\", \"ETL\",\n",
    "    \"Extract Transform Load\", \"Informatica\", \"Talend\", \"SSIS\", \"CRM\",\n",
    "    \"Customer Relationship Management\", \"Salesforce\", \"SAP\", \"Git\", \"NoSQL\", \"MongoDB\",\n",
    "    \"Cassandra\", \"PostgreSQL\", \"MySQL\", \"Data Modeling\", \"Machine Learning\", \"ML\", \"AI\",\n",
    "    \"Apache Kafka\", \"Redis\", \"Elasticsearch\", \"Kibana\", \"Ansible\", \"REST\", \"RESTful\", \"API\",\n",
    "    \"GraphQL\", \"Linux\", \"Matplotlib\", \"Seaborn\", \"Jupyter Notebook\", \"Scikit-learn\",\n",
    "    \"TensorFlow\", \"PyTorch\", \"Data Lakes\", \"Data Warehousing\", \"Agile\", \"Scrum\", \"Blockchain\",\n",
    "    \"Edge Computing\", \"VMware\", \"SAS\", \"Flask\", \"Django\", \"Apache\", \"Airflow\", \"Luigi\", \"NLP\",\n",
    "    \"Databricks\", \"redshift\", \"Excel\", \"HANA\", \"Oracle\", \"crypto\", \"BigQuery\", \"DataGovernance\"\n",
    "]\n",
    "\n",
    "keywords_site = ['remote', 'hybrid', 'on-site', \"flexible\"]\n",
    "# Function to categorize job titles and descriptions by keywords, accepting a keyword list as a parameter\n",
    "def categorize_by_keywords(text, keywords):\n",
    "    doc = nlp(text)\n",
    "    # Initialize an empty set to avoid duplicates\n",
    "    keywords_found = set()\n",
    "    # Check each token in the text\n",
    "    for token in doc:\n",
    "        # Normalize the token's text for case-insensitive matching\n",
    "        token_text = token.text.lower()\n",
    "        # If the normalized token is in our list of keywords, add the original token text to the set\n",
    "        if token_text in [keyword.lower() for keyword in keywords]:\n",
    "            keywords_found.add(token.text)\n",
    "    # Return a comma-separated string of unique keywords found, or \"None\" if no keywords were identified\n",
    "    return ', '.join(keywords_found) if keywords_found else \"None\"\n",
    "\n",
    "# Applying the function to each row for both jobSkills and jobSite columns\n",
    "df_ok['jobSkills'] = df_ok.apply(lambda row: categorize_by_keywords(row['Job-Tags'], keywords_skills), axis=1)\n",
    "df_ok['jobSite'] = df_ok.apply(lambda row: categorize_by_keywords(row['Job-Type'], keywords_site), axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize titles\n",
    "keywords_title = {\n",
    "    \"data administrator\": [\"data\", \"administrator\", \"entry\", \"protection\", \"officer\", \"clerk\", \"admin\", \"migration\", \"cleanser\", \"inputter\", \"coordinator\", \"assistant\", \"pocessor\", \"auditor\", \"governance\", \"apprentice\", \"executive\", \"manager\"],\n",
    "    \"data engineer\": [\"data\", \"engineer\", \"developer\", \"engineering\", \"modeller\", \"technical\"],\n",
    "    \"data analyst\": [\"data\", \"analyst\", \"analytics\", \"analysis\", \"investigation\", \"workstream\", \"visualisation\", \"insight\", \"consultant\"],\n",
    "    \"database administrator\": [\"database\", \"administrator\", \"assistant\", \"manager\"],\n",
    "    \"data scientist\": [\"data\", \"scientist\", \"science\", \"engineer\"],\n",
    "    \"data center\": [\"data\", \"center\", \"cabling\", \"installer\", \"installation\", \"engineer\"],\n",
    "    \"data test\": [\"data\", \"test\", \"tester\", \"automation\", \"processing\"],\n",
    "    \"data architect\": [\"data\", \"architect\"],\n",
    "    \"manager\": [\"head\", \"manager\", \"director\", \"procurement\", \"management\"]  \n",
    "}\n",
    "def categorize_job_titles(job_title, keywords_title):\n",
    "    # Prepare the job title: lowercase, remove special characters, and split into words\n",
    "    words = re.sub('[^a-z0-9\\s]', '', job_title.lower()).split()\n",
    "    \n",
    "    # Initialize a dictionary to hold the count of matches for each category\n",
    "    matches = defaultdict(int)\n",
    "    # Initialize a dictionary to hold the sum of indexes for matched words for tie-breaking\n",
    "    index_sums = defaultdict(int)\n",
    "    \n",
    "    for category, keywords in keywords_title.items():\n",
    "        for word in words:\n",
    "            if word in keywords:\n",
    "                matches[category] += 1\n",
    "                # Sum the indexes of matched words for tie-breaking\n",
    "                index_sums[category] += keywords.index(word)\n",
    "    \n",
    "    if not matches:\n",
    "        return 'Other'\n",
    "    \n",
    "    # Find the category(ies) with the maximum count of matches\n",
    "    max_matches = max(matches.values())\n",
    "    candidates = [category for category, count in matches.items() if count == max_matches]\n",
    "    \n",
    "    # If there's a single best match, return it\n",
    "    if len(candidates) == 1:\n",
    "        return candidates[0]\n",
    "    \n",
    "    # If there are ties, use the sum of indexes for tie-breaking\n",
    "    return min(candidates, key=lambda category: index_sums[category])\n",
    "# Apply the function to the 'jobTitle' column and create a new 'jobCategory' column\n",
    "#df_ok['jobCategory'] = df_ok['Job-Category'].apply(lambda x: categorize_job_titles(x, keywords_title))\n",
    "df_ok['jobCategory'] = df_ok.apply(\n",
    "    lambda row: categorize_job_titles(row['Job-Title'] + \" \" + row['Job-Category'], keywords_title), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok = df_ok[['jobCategory', 'jobLevel', 'jobSkills', 'jobSite', 'Salary-Min', 'Salary-Max', 'sourceId','Job-Title','LinkedIn-Job-Link','Location' ]]\n",
    "df_ok.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_ok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sort_and_deduplicate(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Lowercase, strip whitespace, and split on commas\n",
    "    parts = text.lower().strip().split(',')\n",
    "    # Remove duplicates and sort\n",
    "    cleaned_parts = sorted(set(part.strip() for part in parts))\n",
    "    # Join the cleaned parts back into a single string\n",
    "    return ', '.join(cleaned_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the modified function to sort the terms within the 'jobSite' column using .loc\n",
    "df_ok.loc[:, 'jobSite'] = df_ok['jobSite'].apply(clean_sort_and_deduplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of source column names to postgres db column names\n",
    "column_mappings = {\n",
    "    'sourceId': 'source_id',\n",
    "    'Job-Title': 'job_title_name',\n",
    "    'jobLevel': 'experience_level',\n",
    "    'Salary-Min': 'salary_min',\n",
    "    'Salary-Max': 'salary_max',\n",
    "    'LinkedIn-Job-Link': 'joboffer_url',\n",
    "    'Location': 'location_country',\n",
    "    'jobSite': 'job_site',\n",
    "    'jobSkills': 'skills',\n",
    "    'jobCategory': 'categories'\n",
    "}\n",
    "additional_columns = {\n",
    "    'data_source_name': 'ok'\n",
    "}\n",
    "# Rename columns based on the mapping\n",
    "df_ok.rename(columns=column_mappings, inplace=True)\n",
    "df_ok.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ok_postgres = df_ok[\n",
    "    [column_mappings.get(col, col) for col in column_mappings.keys()]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional columns with default values\n",
    "for col, default_value in additional_columns.items():\n",
    "    df_ok_postgres[col] = default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill salary \n",
    "\n",
    "df_ok_postgres['salary_min'] = df_ok_postgres.groupby('categories')['salary_min'].transform(lambda x: x.fillna(x.mean()))\n",
    "df_ok_postgres['salary_max'] = df_ok_postgres.groupby('categories')['salary_max'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ok_postgres.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure salaries are integers and greater than 0\n",
    "df_ok_postgres['salary_min'] = pd.to_numeric(df_ok_postgres['salary_min'], errors='coerce').fillna(0).astype(int)\n",
    "df_ok_postgres['salary_max'] = pd.to_numeric(df_ok_postgres['salary_max'], errors='coerce').fillna(0).astype(int)\n",
    "df_ok_postgres = df_ok_postgres[(df_ok_postgres['salary_min'] > 0) & (df_ok_postgres['salary_max'] > 0)]\n",
    "# Set the types for other fields as strings\n",
    "df_ok_postgres['source_id'] = df_ok_postgres['source_id'].astype(str)\n",
    "df_ok_postgres['experience_level'] = df_ok_postgres['experience_level'].astype(str)\n",
    "df_ok_postgres['joboffer_url'] = df_ok_postgres['joboffer_url'].astype(str)\n",
    "#df_ok_postgres['currency_symbol'] = df_ok_postgres['currency_symbol'].astype(str)\n",
    "df_ok_postgres['location_country'] = df_ok_postgres['location_country'].astype(str)\n",
    "df_ok_postgres['data_source_name'] = df_ok_postgres['data_source_name'].astype(str)\n",
    "#df_ok_postgres['skills'] = df_ok_postgres['skills'].astype(list)\n",
    "#df_ok_postgres['categories'] = df_ok_postgres['categories'].astype(list)\n",
    "df_ok_postgres['job_site'] = df_ok_postgres['job_site'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for country column\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "def city_to_country(city):\n",
    "    try:\n",
    "        # Geolocate the city\n",
    "        location = geolocator.geocode(city)\n",
    "        # Return the country\n",
    "        return location.address.split(',')[-1]\n",
    "    except:\n",
    "        # Return None if the city can't be geolocated\n",
    "        return None\n",
    "\n",
    "df_ok_postgres['location_country'] = df_ok_postgres['location_country'].apply(city_to_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for currency and published date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
